# GPT-J
name = EleutherAI GPT-J 6B
transformer.type = ELEUTHERAI_NEO

# Main dimensions
hidden.size = 2560
feedforward.size = 10240
decoder.count = 28
attention.head.count = 16
epsilon = 1e-5f
attention.local.size = 256

# Tokenizer config
tokenizer = GPT-2
tokenizer.config = GPT-J
token.count = 50257
end.of.text.token = 50256
max.length = 2048


# Comments on file mappings:

# It seems the mlp normalization weights and biases are common, at least there's no such files for every decoder.
# There are two files: 0 ("ln_f.weight") and 1 ("ln_f.bias"), I presume these are for the mlp normalization.
# 2 ("lm_head.weight"), 3 ("lm_head.bias")

# There's no "input/wpe" file, because instead of the learned position embedding it uses the Rotary Position Embedding (RoPE)
