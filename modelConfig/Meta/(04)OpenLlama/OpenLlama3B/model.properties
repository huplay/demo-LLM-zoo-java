# Llama2/OpenLlama3B
name = OpenLlama 3B
transformer.type = META_LLAMA

# Main dimensions
hidden.size = 3200
feedforward.size = 8640
decoder.count = 26
attention.head.count = 32
epsilon = 1e-6f

# Tokenizer config
tokenizer = SentencePiece
tokenizer.config = Llama1-2
token.count = 32000
end.of.text.token = 2
max.length = 2048

# Parameter config
repo.url = https://huggingface.co/openlm-research/open_llama_3b_v2
repo.branch = refs/pr/16, main
parameter.files = model.safetensors
transformer.parameter.format = {name}
decoder.parameter.format = model.layers.{decoderId}.{name}
transformer.parameter.overrides = norm.weight : lm_head.weight, embed_tokens.weight : model.embed_tokens.weight
decoder.parameter.overrides =
