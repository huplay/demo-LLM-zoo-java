# Llama2/tinyLlama15M
name = Karpathy tinyLlama 15M
transformer.type = META_LLAMA

# Main dimensions
hidden.size = 288
feedforward.size = 768
decoder.count = 6
attention.head.count = 6
epsilon = 1e-5f

# Tokenizer config
tokenizer = SentencePiece
tokenizer.config = Llama1-2
token.count = 32000
end.of.text.token = 2
max.length = 256

# Parameter config
# Original repo of Andrej Karpathy (no safetensors format): https://huggingface.co/karpathy/tinyllamas/tree/main
parameter.repo = https://huggingface.co/nickypro/tinyllama-15M
parameter.repo.branch = refs%2Fpr%2F2
parameter.files = model.safetensors
transformer.parameter.format = model.{name}
decoder.parameter.format = model.layers.{decoderId}.{name}
