# Llama2/7B
name = Meta Llama2 7B
transformer.type = META_LLAMA

# Main dimensions
hidden.size = 4096
feedforward.size = 11008
decoder.count = 32
attention.head.count = 32
epsilon = 1e-5f

# Tokenizer config
tokenizer = SentencePiece
tokenizer.config = Llama1-2
token.count = 32000
end.of.text.token = 2
max.length = 256

# Parameter config
parameter.files = model-00001-of-00002.safetensors, model-00002-of-00002.safetensors
transformer.parameter.format = model.{name}
decoder.parameter.format = model.layers.{decoderId}.{name}
