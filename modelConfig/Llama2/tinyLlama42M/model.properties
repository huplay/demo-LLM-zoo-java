# Llama2/tinyLlama42M
name = Karpathy tinyLlama 42M
transformer.type = META_LLAMA

# Main dimensions
hidden.size = 512
feedforward.size = 1376
decoder.count = 8
attention.head.count = 8
epsilon = 1e-5f

# Tokenizer config
tokenizer = SentencePiece
tokenizer.config = Llama1-2
token.count = 32000
end.of.text.token = 2
max.length = 1024

# Parameter config

# Original repo of Andrej Karpathy (no safetensors format): https://huggingface.co/karpathy/tinyllamas
# nickypro user converted the parameters to llama2.c format (the steps are described by Karpathy),
# and the converted file was converted automatically by HuggingFace to safeternsors format.
# At the moment it is only a pull request, isn't merged. But most likely it will happen soon.
# https://huggingface.co/nickypro/tinyllama-42M
parameter.url = https://huggingface.co/nickypro/tinyllama-42M/resolve/refs%2Fpr%2F1/model.safetensors?download=true
parameter.files = model.safetensors
transformer.parameter.format = model.{name}
decoder.parameter.format = model.layers.{decoderId}.{name}

