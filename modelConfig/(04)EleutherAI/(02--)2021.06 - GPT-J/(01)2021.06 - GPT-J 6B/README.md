# GPT-J-6B parameters

(EleutherAI GPT-J-6B)

---

Uses Rotary Position Embedding (RoPE)

Publication: https://arxiv.org/abs/2104.09864 (20 Apr 2021, Jianlin Su et al.)

Original implementation (RoFormer, 22 Mar 2021, Jianlin Su et al.): https://github.com/ZhuiyiTechnology/roformer

---

Pytorch implementation (29 Jun - 16 Aug 2021): https://github.com/lucidrains/rotary-embedding-torch

GPT-J-6B implementation (31 Aug 2021, using Hugging Face repo): https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/gptj/modeling_gptj.py

Hugging Face documentation (RoFormer): https://huggingface.co/docs/transformers/model_doc/roformer

---

Source of the parameters: https://huggingface.co/EleutherAI/gpt-j-6B


# It seems the mlp normalization weights and biases are common, at least there's no such files for every decoder.



